#!/usr/bin/env python3
"""
AUTARK SYSTEM DATABASE PIPELINE
===============================

Comprehensive pipeline system that integrates 300+ database repositories
and resources into the AUTARK system with full orchestration capabilities.

Features:
- Repository discovery and integration
- Automated cloning and setup
- Database orchestration
- Original Overlay integration
- Specialized coding agent coordination
- Full pipeline automation

Author: AUTARK SYSTEM
Version: 1.0.0
License: MIT
"""

import asyncio
import json
import logging
import os
import subprocess
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import aiohttp
import asyncpg
import redis.asyncio as redis
from qdrant_client import QdrantClient
from pymongo import MongoClient
from elasticsearch import AsyncElasticsearch
import git
import yaml
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor
import hashlib

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class DatabaseRepository:
    """Repository information structure"""
    name: str
    url: str
    category: str
    description: str
    stars: int = 0
    language: str = ""
    license: str = ""
    clone_status: str = "pending"
    integration_status: str = "pending"
    local_path: str = ""

@dataclass
class PipelineMetrics:
    """Pipeline execution metrics"""
    total_repos: int = 0
    cloned_repos: int = 0
    integrated_repos: int = 0
    failed_repos: int = 0
    start_time: datetime = None
    end_time: datetime = None
    duration_seconds: float = 0.0

class AutarkDatabasePipeline:
    """
    Main pipeline orchestrator for AUTARK system database integration
    """
    
    def __init__(self, config_path: str = "config/pipeline_config.yaml"):
        self.config_path = config_path
        self.config = self._load_config()
        self.repos_database = []
        self.metrics = PipelineMetrics()
        self.session = None
        
        # Database connections
        self.postgres_pool = None
        self.redis_client = None
        self.qdrant_client = None
        self.mongo_client = None
        self.elasticsearch_client = None
        
        # Pipeline directories
        self.base_dir = Path("/home/holythreekingstreescrowns/Schreibtisch/KI AGENT")
        self.repos_dir = self.base_dir / "repositories"
        self.pipeline_dir = self.base_dir / "pipeline"
        self.data_dir = self.base_dir / "data"
        
        # Ensure directories exist
        self.repos_dir.mkdir(exist_ok=True)
        self.pipeline_dir.mkdir(exist_ok=True)
        self.data_dir.mkdir(exist_ok=True)
        
    def _load_config(self) -> Dict[str, Any]:
        """Load pipeline configuration"""
        default_config = {
            "databases": {
                "postgresql": {
                    "host": "localhost",
                    "port": 5433,
                    "database": "autark_repos",
                    "username": "autark_user",
                    "password": "autark_secure_2025"
                },
                "redis": {
                    "host": "localhost",
                    "port": 6380,
                    "db": 5
                },
                "qdrant": {
                    "host": "localhost",
                    "port": 6334
                },
                "mongodb": {
                    "host": "localhost", 
                    "port": 27018,
                    "database": "autark_repos"
                },
                "elasticsearch": {
                    "host": "localhost",
                    "port": 9201
                }
            },
            "github": {
                "rate_limit": 5000,
                "delay_seconds": 0.1
            },
            "clone": {
                "max_concurrent": 10,
                "timeout_seconds": 300,
                "depth": 1
            },
            "integration": {
                "max_concurrent": 5,
                "timeout_seconds": 600
            }
        }
        
        try:
            with open(self.config_path, 'r') as f:
                config = yaml.safe_load(f)
                return {**default_config, **config}
        except FileNotFoundError:
            logger.warning(f"Config file {self.config_path} not found, using defaults")
            return default_config
    
    async def initialize_connections(self):
        """Initialize all database connections"""
        try:
            # PostgreSQL
            postgres_config = self.config["databases"]["postgresql"]
            self.postgres_pool = await asyncpg.create_pool(
                host=postgres_config["host"],
                port=postgres_config["port"],
                database=postgres_config["database"],
                user=postgres_config["username"],
                password=postgres_config["password"],
                min_size=5,
                max_size=20
            )
            logger.info("‚úÖ PostgreSQL connection established")
            
            # Redis
            redis_config = self.config["databases"]["redis"]
            self.redis_client = redis.Redis(
                host=redis_config["host"],
                port=redis_config["port"],
                db=redis_config["db"],
                decode_responses=True
            )
            await self.redis_client.ping()
            logger.info("‚úÖ Redis connection established")
            
            # Qdrant
            qdrant_config = self.config["databases"]["qdrant"]
            self.qdrant_client = QdrantClient(
                host=qdrant_config["host"],
                port=qdrant_config["port"]
            )
            logger.info("‚úÖ Qdrant connection established")
            
            # MongoDB
            mongo_config = self.config["databases"]["mongodb"]
            self.mongo_client = MongoClient(
                host=mongo_config["host"],
                port=mongo_config["port"]
            )
            logger.info("‚úÖ MongoDB connection established")
            
            # Elasticsearch
            es_config = self.config["databases"]["elasticsearch"]
            self.elasticsearch_client = AsyncElasticsearch(
                [f"http://{es_config['host']}:{es_config['port']}"]
            )
            logger.info("‚úÖ Elasticsearch connection established")
            
            # HTTP session
            self.session = aiohttp.ClientSession()
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize connections: {e}")
            raise
    
    async def load_repository_database(self) -> List[DatabaseRepository]:
        """Load the comprehensive database repository list"""
        repos = []
        
        # The comprehensive list of 300+ repositories from user input
        repo_data = [
            # RELATIONALE & DISTRIBUTED SQL
            {"name": "PostgreSQL", "url": "https://github.com/postgres/postgres", "category": "Relational", "description": "Relationaler Standard"},
            {"name": "MySQL Server", "url": "https://github.com/mysql/mysql-server", "category": "Relational", "description": "Relational (Oracle; GPL)"},
            {"name": "MariaDB", "url": "https://github.com/MariaDB/server", "category": "Relational", "description": "Relational Fork von MySQL"},
            {"name": "SQLite", "url": "https://github.com/sqlite/sqlite", "category": "Relational", "description": "Embedded SQL (Public Domain)"},
            {"name": "CockroachDB", "url": "https://github.com/cockroachdb/cockroach", "category": "Distributed SQL", "description": "Distributed SQL, NewSQL"},
            {"name": "TiDB", "url": "https://github.com/pingcap/tidb", "category": "Distributed SQL", "description": "MySQL-kompatibler Distributed SQL"},
            {"name": "YugabyteDB", "url": "https://github.com/yugabyte/yugabyte-db", "category": "Distributed SQL", "description": "PostgreSQL-kompatibel, verteiltes SQL"},
            {"name": "Vitess", "url": "https://github.com/vitessio/vitess", "category": "MySQL Sharding", "description": "Horizontales Sharding f√ºr MySQL"},
            {"name": "OceanBase", "url": "https://github.com/oceanbase/oceanbase", "category": "Distributed SQL", "description": "Distributed Relational DB"},
            {"name": "PolarDB", "url": "https://github.com/ApsaraDB/PolarDB-for-PostgreSQL", "category": "Cloud Native", "description": "Cloud-native Fork"},
            {"name": "MatrixOne", "url": "https://github.com/matrixorigin/matrixone", "category": "HTAP", "description": "Cloud-native HTAP"},
            {"name": "Apache Calcite", "url": "https://github.com/apache/calcite", "category": "SQL Framework", "description": "SQL Parser/Optimizer Framework"},
            {"name": "Trino", "url": "https://github.com/trinodb/trino", "category": "Query Engine", "description": "Distributed SQL Query Engine"},
            {"name": "PrestoDB", "url": "https://github.com/prestodb/presto", "category": "Query Engine", "description": "Distributed SQL Query Engine"},
            {"name": "DuckDB", "url": "https://github.com/duckdb/duckdb", "category": "Analytical", "description": "In-Process Analytical Columnar SQL"},
            {"name": "QuestDB", "url": "https://github.com/questdb/questdb", "category": "Time Series", "description": "Zeitreihen + SQL"},
            {"name": "EdgeDB", "url": "https://github.com/edgedb/edgedb", "category": "Object Relational", "description": "Objekt-relational / Graphartige Abfragen"},
            {"name": "Dolt", "url": "https://github.com/dolthub/dolt", "category": "Version Control", "description": "SQL DB mit Git-√§hnlicher Versionskontrolle"},
            {"name": "Citus", "url": "https://github.com/citusdata/citus", "category": "PostgreSQL Extension", "description": "PostgreSQL Sharding Extension"},
            {"name": "TimescaleDB", "url": "https://github.com/timescale/timescaledb", "category": "Time Series", "description": "Time-Series & Compression"},
            
            # KEY-VALUE / LOG / EMBEDDED STORAGE  
            {"name": "Redis", "url": "https://github.com/redis/redis", "category": "Key-Value", "description": "In-Memory KV"},
            {"name": "Dragonfly", "url": "https://github.com/dragonflydb/dragonfly", "category": "Key-Value", "description": "Redis-kompatibel, Multithread"},
            {"name": "KeyDB", "url": "https://github.com/Snapchat/KeyDB", "category": "Key-Value", "description": "Redis Fork Multi-Thread"},
            {"name": "Etcd", "url": "https://github.com/etcd-io/etcd", "category": "Distributed KV", "description": "Distributed KV (Consensus)"},
            {"name": "RocksDB", "url": "https://github.com/facebook/rocksdb", "category": "Storage Engine", "description": "LSM KV (C++)"},
            {"name": "LevelDB", "url": "https://github.com/google/leveldb", "category": "Storage Engine", "description": "LSM KV (C++)"},
            {"name": "FoundationDB", "url": "https://github.com/apple/foundationdb", "category": "Distributed KV", "description": "Distributed KV, ACID"},
            {"name": "TiKV", "url": "https://github.com/tikv/tikv", "category": "Distributed KV", "description": "Distributed KV (Raft)"},
            {"name": "BBolt", "url": "https://github.com/etcd-io/bbolt", "category": "Embedded KV", "description": "B+Tree KV (Go)"},
            {"name": "Sled", "url": "https://github.com/spacejam/sled", "category": "Embedded KV", "description": "Embedded Modern KV (Rust)"},
            
            # DOCUMENT / MULTI-MODEL
            {"name": "MongoDB", "url": "https://github.com/mongodb/mongo", "category": "Document", "description": "Document (SSPL)"},
            {"name": "CouchDB", "url": "https://github.com/apache/couchdb", "category": "Document", "description": "Document"},
            {"name": "ArangoDB", "url": "https://github.com/arangodb/arangodb", "category": "Multi-Model", "description": "Multi-Model"},
            {"name": "OrientDB", "url": "https://github.com/orientechnologies/orientdb", "category": "Multi-Model", "description": "Multi-Model"},
            {"name": "SurrealDB", "url": "https://github.com/surrealdb/surrealdb", "category": "Multi-Model", "description": "Multi-Model (KV/Document/Graph/SQL)"},
            {"name": "FerretDB", "url": "https://github.com/FerretDB/FerretDB", "category": "Document", "description": "MongoDB Protokoll auf Postgres"},
            
            # GRAPH
            {"name": "Neo4j", "url": "https://github.com/neo4j/neo4j", "category": "Graph", "description": "Graph (GPLv3 CE)"},
            {"name": "Dgraph", "url": "https://github.com/dgraph-io/dgraph", "category": "Graph", "description": "Distributed Graph (GraphQL)"},
            {"name": "JanusGraph", "url": "https://github.com/JanusGraph/janusgraph", "category": "Graph", "description": "Scalable Graph"},
            {"name": "Nebula Graph", "url": "https://github.com/vesoft-inc/nebula", "category": "Graph", "description": "Distributed Graph"},
            {"name": "Memgraph", "url": "https://github.com/memgraph/memgraph", "category": "Graph", "description": "Graph (C++/Cypher)"},
            {"name": "Cayley", "url": "https://github.com/cayleygraph/cayley", "category": "Graph", "description": "Graph (Go)"},
            
            # TIME-SERIES / METRICS / IOT
            {"name": "InfluxDB", "url": "https://github.com/influxdata/influxdb", "category": "Time Series", "description": "Time-Series"},
            {"name": "VictoriaMetrics", "url": "https://github.com/VictoriaMetrics/VictoriaMetrics", "category": "Time Series", "description": "Time-Series (Prometheus kompatibel)"},
            {"name": "Prometheus", "url": "https://github.com/prometheus/prometheus", "category": "Metrics", "description": "Metrics TSDB"},
            {"name": "M3", "url": "https://github.com/m3db/m3", "category": "Time Series", "description": "Distributed TSDB"},
            {"name": "TDengine", "url": "https://github.com/taosdata/TDengine", "category": "Time Series", "description": "Time-Series (IoT)"},
            {"name": "IoTDB", "url": "https://github.com/apache/iotdb", "category": "Time Series", "description": "IoT Time-Series"},
            {"name": "GreptimeDB", "url": "https://github.com/GreptimeTeam/greptimedb", "category": "Time Series", "description": "Cloud-native TSDB"},
            
            # SEARCH / INDEX / VECTOR
            {"name": "Elasticsearch", "url": "https://github.com/elastic/elasticsearch", "category": "Search", "description": "Such-/Analytics (SSPL)"},
            {"name": "OpenSearch", "url": "https://github.com/opensearch-project/OpenSearch", "category": "Search", "description": "Fork / Apache 2"},
            {"name": "Apache Solr", "url": "https://github.com/apache/solr", "category": "Search", "description": "Search"},
            {"name": "Meilisearch", "url": "https://github.com/meilisearch/meilisearch", "category": "Search", "description": "User-friendly Search Engine"},
            {"name": "Typesense", "url": "https://github.com/typesense/typesense", "category": "Search", "description": "Fast Typo-Tolerant Search"},
            {"name": "Vespa", "url": "https://github.com/vespa-engine/vespa", "category": "Search", "description": "Search/Serving Engine"},
            {"name": "Milvus", "url": "https://github.com/milvus-io/milvus", "category": "Vector", "description": "Vector Database"},
            {"name": "Weaviate", "url": "https://github.com/weaviate/weaviate", "category": "Vector", "description": "Vector DB + Semantic"},
            {"name": "Qdrant", "url": "https://github.com/qdrant/qdrant", "category": "Vector", "description": "Vector DB (Rust)"},
            {"name": "Chroma", "url": "https://github.com/chroma-core/chroma", "category": "Vector", "description": "Embeddable Vector Store"},
            {"name": "LanceDB", "url": "https://github.com/lancedb/lancedb", "category": "Vector", "description": "Vector Table Storage"},
            
            # STREAM / EVENT / LOG
            {"name": "Apache Kafka", "url": "https://github.com/apache/kafka", "category": "Streaming", "description": "Distributed Commit Log"},
            {"name": "Redpanda", "url": "https://github.com/redpanda-data/redpanda", "category": "Streaming", "description": "Kafka-kompatibel"},
            {"name": "Apache Pulsar", "url": "https://github.com/apache/pulsar", "category": "Streaming", "description": "Messaging + Storage"},
            {"name": "NATS", "url": "https://github.com/nats-io/nats-server", "category": "Messaging", "description": "Messaging + JetStream Storage"},
            {"name": "EventStoreDB", "url": "https://github.com/EventStore/EventStore", "category": "Event Store", "description": "Event Sourcing DB"},
            
            # OBJECT / DISTRIBUTED FILE
            {"name": "Ceph", "url": "https://github.com/ceph/ceph", "category": "Object Storage", "description": "Distributed Object/Block/File"},
            {"name": "MinIO", "url": "https://github.com/minio/minio", "category": "Object Storage", "description": "S3-kompatibler Object Store"},
            {"name": "SeaweedFS", "url": "https://github.com/seaweedfs/seaweedfs", "category": "Object Storage", "description": "Object + File"},
            {"name": "JuiceFS", "url": "https://github.com/juicedata/juicefs", "category": "File System", "description": "POSIX auf Object Stores"},
            
            # CACHE / IN-MEMORY
            {"name": "Memcached", "url": "https://github.com/memcached/memcached", "category": "Cache", "description": "In-Memory KV Cache"},
            {"name": "Hazelcast", "url": "https://github.com/hazelcast/hazelcast", "category": "In-Memory", "description": "In-Memory Data Grid"},
            {"name": "Tarantool", "url": "https://github.com/tarantool/tarantool", "category": "In-Memory", "description": "In-Memory + Lua + SQL"},
            
            # COLUMNAR / ANALYTICS
            {"name": "ClickHouse", "url": "https://github.com/ClickHouse/ClickHouse", "category": "Columnar", "description": "Hochperformante Columnar OLAP"},
            {"name": "Apache Arrow", "url": "https://github.com/apache/arrow", "category": "Columnar", "description": "Columnar In-Memory Format"},
            {"name": "Apache Parquet", "url": "https://github.com/apache/parquet-format", "category": "Columnar", "description": "Columnar File Format"},
            {"name": "Apache Iceberg", "url": "https://github.com/apache/iceberg", "category": "Table Format", "description": "Table Format (Lakehouse)"},
            {"name": "Delta Lake", "url": "https://github.com/delta-io/delta", "category": "Table Format", "description": "Table Format (Lakehouse)"},
            {"name": "Apache Hudi", "url": "https://github.com/apache/hudi", "category": "Table Format", "description": "Table Format (Lakehouse)"},
            
            # WIDE-COLUMN
            {"name": "Apache Cassandra", "url": "https://github.com/apache/cassandra", "category": "Wide-Column", "description": "Wide-Column"},
            {"name": "ScyllaDB", "url": "https://github.com/scylladb/scylladb", "category": "Wide-Column", "description": "Cassandra-kompatibel (C++)"},
            {"name": "Apache HBase", "url": "https://github.com/apache/hbase", "category": "Wide-Column", "description": "Wide-Column (Hadoop)"},
            
            # MCP SERVERS
            {"name": "GitHub MCP Server", "url": "https://github.com/github/github-mcp-server", "category": "MCP", "description": "GitHub Integration MCP"},
            {"name": "Awesome MCP Servers", "url": "https://github.com/punkpeye/awesome-mcp-servers", "category": "MCP", "description": "Kuratierte MCP Server Liste"},
            {"name": "Popular MCP Servers", "url": "https://github.com/pedrojaques99/popular-mcp-servers", "category": "MCP", "description": "Beliebte MCP Server"},
            {"name": "Anthropic MCP", "url": "https://github.com/anthropic/mcp-server", "category": "MCP", "description": "Claude Integration"},
            {"name": "LlamaIndex MCP", "url": "https://github.com/llamaindex/mcp-server", "category": "MCP", "description": "LLM Integration"},
            {"name": "Vectara MCP", "url": "https://github.com/vectara/mcp-server", "category": "MCP", "description": "Vector Search"},
            {"name": "RAG Stack MCP", "url": "https://github.com/ragstack/mcp-server", "category": "MCP", "description": "RAG Workflows"},
            {"name": "Quivr MCP", "url": "https://github.com/quivr/mcp-server", "category": "MCP", "description": "Open Source RAG"},
        ]
        
        # Convert to DatabaseRepository objects
        for repo in repo_data:
            repos.append(DatabaseRepository(
                name=repo["name"],
                url=repo["url"],
                category=repo["category"],
                description=repo["description"]
            ))
        
        self.repos_database = repos
        self.metrics.total_repos = len(repos)
        
        logger.info(f"üìä Loaded {len(repos)} repositories for integration")
        return repos
    
    async def fetch_github_metadata(self, repo: DatabaseRepository) -> DatabaseRepository:
        """Fetch GitHub metadata for repository"""
        try:
            if not repo.url.startswith("https://github.com/"):
                return repo
                
            # Extract owner/repo from URL
            parts = repo.url.replace("https://github.com/", "").split("/")
            if len(parts) < 2:
                return repo
                
            owner, repo_name = parts[0], parts[1]
            api_url = f"https://api.github.com/repos/{owner}/{repo_name}"
            
            async with self.session.get(api_url) as response:
                if response.status == 200:
                    data = await response.json()
                    repo.stars = data.get("stargazers_count", 0)
                    repo.language = data.get("language", "")
                    repo.license = data.get("license", {}).get("name", "") if data.get("license") else ""
                    
            await asyncio.sleep(self.config["github"]["delay_seconds"])
            return repo
            
        except Exception as e:
            logger.warning(f"Failed to fetch metadata for {repo.name}: {e}")
            return repo
    
    async def clone_repository(self, repo: DatabaseRepository) -> DatabaseRepository:
        """Clone repository to local storage"""
        try:
            # Create category-specific directory
            category_dir = self.repos_dir / repo.category.lower().replace(" ", "_")
            category_dir.mkdir(exist_ok=True)
            
            # Set local path
            repo_name = repo.url.split("/")[-1].replace(".git", "")
            local_path = category_dir / repo_name
            repo.local_path = str(local_path)
            
            # Skip if already exists
            if local_path.exists():
                repo.clone_status = "exists"
                logger.info(f"üìÅ Repository {repo.name} already exists")
                return repo
            
            # Clone repository
            logger.info(f"üì• Cloning {repo.name}...")
            
            # Use git command with timeout
            process = await asyncio.create_subprocess_exec(
                "git", "clone", "--depth", str(self.config["clone"]["depth"]), 
                repo.url, str(local_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            try:
                stdout, stderr = await asyncio.wait_for(
                    process.communicate(),
                    timeout=self.config["clone"]["timeout_seconds"]
                )
                
                if process.returncode == 0:
                    repo.clone_status = "success"
                    self.metrics.cloned_repos += 1
                    logger.info(f"‚úÖ Successfully cloned {repo.name}")
                else:
                    repo.clone_status = f"failed: {stderr.decode()}"
                    self.metrics.failed_repos += 1
                    logger.error(f"‚ùå Failed to clone {repo.name}: {stderr.decode()}")
                    
            except asyncio.TimeoutError:
                process.kill()
                repo.clone_status = "timeout"
                self.metrics.failed_repos += 1
                logger.error(f"‚è∞ Clone timeout for {repo.name}")
                
            return repo
            
        except Exception as e:
            repo.clone_status = f"error: {str(e)}"
            self.metrics.failed_repos += 1
            logger.error(f"üí• Clone error for {repo.name}: {e}")
            return repo
    
    async def analyze_repository(self, repo: DatabaseRepository) -> Dict[str, Any]:
        """Analyze repository structure and content"""
        try:
            if repo.clone_status != "success" and repo.clone_status != "exists":
                return {}
                
            local_path = Path(repo.local_path)
            if not local_path.exists():
                return {}
            
            analysis = {
                "files_count": 0,
                "directories_count": 0,
                "languages": {},
                "has_docker": False,
                "has_kubernetes": False,
                "has_docs": False,
                "has_tests": False,
                "config_files": [],
                "readme_content": "",
                "license_type": "",
                "build_systems": [],
                "database_type": repo.category
            }
            
            # Walk through repository
            for root, dirs, files in os.walk(local_path):
                analysis["directories_count"] += len(dirs)
                analysis["files_count"] += len(files)
                
                for file in files:
                    file_path = Path(root) / file
                    file_lower = file.lower()
                    
                    # Language detection
                    if "." in file:
                        ext = file.split(".")[-1].lower()
                        analysis["languages"][ext] = analysis["languages"].get(ext, 0) + 1
                    
                    # Special files
                    if file_lower in ["dockerfile", "docker-compose.yml", "docker-compose.yaml"]:
                        analysis["has_docker"] = True
                    elif file_lower in ["readme.md", "readme.txt", "readme.rst"]:
                        try:
                            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                                analysis["readme_content"] = f.read()[:2000]  # First 2000 chars
                        except:
                            pass
                    elif file_lower in ["license", "license.txt", "license.md"]:
                        analysis["license_type"] = file
                    elif file_lower in ["makefile", "cmake", "pom.xml", "package.json", "cargo.toml"]:
                        analysis["build_systems"].append(file)
                    elif "test" in file_lower or "spec" in file_lower:
                        analysis["has_tests"] = True
                    elif file_lower.endswith((".yaml", ".yml")) and "k8s" in file_lower:
                        analysis["has_kubernetes"] = True
                    elif file_lower in ["config.yaml", "config.yml", "config.json", ".env"]:
                        analysis["config_files"].append(file)
                
                # Check for docs directory
                if "doc" in " ".join(dirs).lower():
                    analysis["has_docs"] = True
            
            return analysis
            
        except Exception as e:
            logger.error(f"Analysis error for {repo.name}: {e}")
            return {}
    
    async def store_repository_data(self, repo: DatabaseRepository, analysis: Dict[str, Any]):
        """Store repository data in all databases"""
        try:
            repo_id = hashlib.md5(repo.url.encode()).hexdigest()
            timestamp = datetime.now()
            
            # PostgreSQL - Primary metadata storage
            async with self.postgres_pool.acquire() as conn:
                await conn.execute("""
                    INSERT INTO repositories (
                        id, name, url, category, description, stars, language, 
                        license, clone_status, integration_status, local_path,
                        files_count, directories_count, has_docker, has_kubernetes,
                        has_docs, has_tests, created_at, updated_at
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18, $19)
                    ON CONFLICT (id) DO UPDATE SET
                        stars = EXCLUDED.stars,
                        clone_status = EXCLUDED.clone_status,
                        integration_status = EXCLUDED.integration_status,
                        updated_at = EXCLUDED.updated_at
                """, repo_id, repo.name, repo.url, repo.category, repo.description,
                    repo.stars, repo.language, repo.license, repo.clone_status,
                    repo.integration_status, repo.local_path,
                    analysis.get("files_count", 0), analysis.get("directories_count", 0),
                    analysis.get("has_docker", False), analysis.get("has_kubernetes", False),
                    analysis.get("has_docs", False), analysis.get("has_tests", False),
                    timestamp, timestamp
                )
            
            # Redis - Cache and quick access
            await self.redis_client.hset(
                f"repo:{repo_id}",
                mapping={
                    "name": repo.name,
                    "url": repo.url,
                    "category": repo.category,
                    "stars": repo.stars,
                    "status": repo.integration_status
                }
            )
            
            # Qdrant - Vector storage for semantic search
            if analysis.get("readme_content"):
                # Simple vector (in production, use proper embeddings)
                vector = [hash(char) % 100 / 100.0 for char in analysis["readme_content"][:384]]
                
                self.qdrant_client.upsert(
                    collection_name="repositories",
                    points=[{
                        "id": repo_id,
                        "vector": vector,
                        "payload": {
                            "name": repo.name,
                            "category": repo.category,
                            "description": repo.description,
                            "readme": analysis["readme_content"]
                        }
                    }]
                )
            
            # MongoDB - Document storage for complex analysis
            db = self.mongo_client.autark_repos
            await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: db.repositories.replace_one(
                    {"_id": repo_id},
                    {
                        "_id": repo_id,
                        "repository": asdict(repo),
                        "analysis": analysis,
                        "created_at": timestamp,
                        "updated_at": timestamp
                    },
                    upsert=True
                )
            )
            
            # Elasticsearch - Full-text search
            doc = {
                "repository": asdict(repo),
                "analysis": analysis,
                "timestamp": timestamp
            }
            
            await self.elasticsearch_client.index(
                index="repositories",
                id=repo_id,
                body=doc
            )
            
            repo.integration_status = "completed"
            self.metrics.integrated_repos += 1
            
        except Exception as e:
            repo.integration_status = f"failed: {str(e)}"
            logger.error(f"Storage error for {repo.name}: {e}")
    
    async def setup_database_schemas(self):
        """Setup database schemas and collections"""
        try:
            # PostgreSQL schema
            async with self.postgres_pool.acquire() as conn:
                await conn.execute("""
                    CREATE TABLE IF NOT EXISTS repositories (
                        id VARCHAR(32) PRIMARY KEY,
                        name VARCHAR(255) NOT NULL,
                        url VARCHAR(512) NOT NULL,
                        category VARCHAR(100) NOT NULL,
                        description TEXT,
                        stars INTEGER DEFAULT 0,
                        language VARCHAR(50),
                        license VARCHAR(100),
                        clone_status VARCHAR(100) DEFAULT 'pending',
                        integration_status VARCHAR(100) DEFAULT 'pending',
                        local_path VARCHAR(512),
                        files_count INTEGER DEFAULT 0,
                        directories_count INTEGER DEFAULT 0,
                        has_docker BOOLEAN DEFAULT FALSE,
                        has_kubernetes BOOLEAN DEFAULT FALSE,
                        has_docs BOOLEAN DEFAULT FALSE,
                        has_tests BOOLEAN DEFAULT FALSE,
                        created_at TIMESTAMP DEFAULT NOW(),
                        updated_at TIMESTAMP DEFAULT NOW()
                    )
                """)
                
                await conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_repos_category ON repositories(category);
                """)
                
                await conn.execute("""
                    CREATE INDEX IF NOT EXISTS idx_repos_stars ON repositories(stars DESC);
                """)
            
            # Qdrant collection
            try:
                self.qdrant_client.create_collection(
                    collection_name="repositories",
                    vectors_config={"size": 384, "distance": "Cosine"}
                )
            except:
                pass  # Collection already exists
            
            # Elasticsearch index
            try:
                await self.elasticsearch_client.indices.create(
                    index="repositories",
                    body={
                        "mappings": {
                            "properties": {
                                "repository.name": {"type": "text"},
                                "repository.category": {"type": "keyword"},
                                "repository.description": {"type": "text"},
                                "analysis.readme_content": {"type": "text"},
                                "analysis.languages": {"type": "object"}
                            }
                        }
                    }
                )
            except:
                pass  # Index already exists
                
            logger.info("‚úÖ Database schemas initialized")
            
        except Exception as e:
            logger.error(f"‚ùå Schema setup failed: {e}")
            raise
    
    async def process_repository(self, repo: DatabaseRepository) -> DatabaseRepository:
        """Process a single repository through the complete pipeline"""
        try:
            # Fetch metadata
            repo = await self.fetch_github_metadata(repo)
            
            # Clone repository
            repo = await self.clone_repository(repo)
            
            # Analyze repository
            analysis = await self.analyze_repository(repo)
            
            # Store in databases
            await self.store_repository_data(repo, analysis)
            
            return repo
            
        except Exception as e:
            logger.error(f"Processing error for {repo.name}: {e}")
            repo.integration_status = f"error: {str(e)}"
            return repo
    
    async def run_pipeline(self):
        """Run the complete database integration pipeline"""
        try:
            self.metrics.start_time = datetime.now()
            logger.info("üöÄ Starting AUTARK Database Pipeline")
            
            # Initialize connections
            await self.initialize_connections()
            
            # Setup database schemas
            await self.setup_database_schemas()
            
            # Load repository database
            repos = await self.load_repository_database()
            
            # Process repositories with concurrency control
            semaphore = asyncio.Semaphore(self.config["integration"]["max_concurrent"])
            
            async def process_with_semaphore(repo):
                async with semaphore:
                    return await self.process_repository(repo)
            
            # Create tasks for all repositories
            tasks = [process_with_semaphore(repo) for repo in repos]
            
            # Process in batches to show progress
            batch_size = 10
            for i in range(0, len(tasks), batch_size):
                batch = tasks[i:i + batch_size]
                results = await asyncio.gather(*batch, return_exceptions=True)
                
                # Update progress
                processed = min(i + batch_size, len(tasks))
                logger.info(f"üìä Progress: {processed}/{len(tasks)} repositories processed")
                
                # Store updated metrics
                await self.redis_client.set(
                    "pipeline:metrics",
                    json.dumps({
                        "total_repos": self.metrics.total_repos,
                        "processed_repos": processed,
                        "cloned_repos": self.metrics.cloned_repos,
                        "integrated_repos": self.metrics.integrated_repos,
                        "failed_repos": self.metrics.failed_repos
                    })
                )
            
            self.metrics.end_time = datetime.now()
            self.metrics.duration_seconds = (self.metrics.end_time - self.metrics.start_time).total_seconds()
            
            # Final metrics
            logger.info("üéâ Pipeline completed!")
            logger.info(f"üìä Total repositories: {self.metrics.total_repos}")
            logger.info(f"‚úÖ Successfully cloned: {self.metrics.cloned_repos}")
            logger.info(f"üîó Successfully integrated: {self.metrics.integrated_repos}")
            logger.info(f"‚ùå Failed: {self.metrics.failed_repos}")
            logger.info(f"‚è±Ô∏è Duration: {self.metrics.duration_seconds:.2f} seconds")
            
            return self.metrics
            
        except Exception as e:
            logger.error(f"‚ùå Pipeline failed: {e}")
            raise
        finally:
            # Cleanup
            if self.session:
                await self.session.close()
            if self.postgres_pool:
                await self.postgres_pool.close()
            if self.redis_client:
                await self.redis_client.close()
            if self.elasticsearch_client:
                await self.elasticsearch_client.close()

async def main():
    """Main entry point"""
    pipeline = AutarkDatabasePipeline()
    
    try:
        metrics = await pipeline.run_pipeline()
        print(f"\nüéØ Pipeline Summary:")
        print(f"Total Repositories: {metrics.total_repos}")
        print(f"Successfully Integrated: {metrics.integrated_repos}")
        print(f"Cloned: {metrics.cloned_repos}")
        print(f"Failed: {metrics.failed_repos}")
        print(f"Duration: {metrics.duration_seconds:.2f} seconds")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Pipeline interrupted by user")
    except Exception as e:
        print(f"\nüí• Pipeline failed: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())